{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuxeCDjWgr5aT6jeAsTp4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busrkayr/eticaretwebsite/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPNknQBLglFk"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import json\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "NJWie77ug6Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Colab GPU kontrolü\n",
        "# ======================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "# Colab ortamında, dosya yolunu \"/content\" olarak ayarla\n",
        "base_path = \"/content\""
      ],
      "metadata": {
        "id": "zIQ80PjchEa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Veri setini yükle\n",
        "dataset = load_dataset(\"winvoker/turkish-sentiment-analysis-dataset\")\n",
        "train_data = dataset[\"train\"].to_pandas()\n",
        "\n",
        "# Her sınıftan 15.000 örnek rastgele seç\n",
        "negative_samples = train_data[train_data[\"label\"] == \"Negative\"].sample(15000, random_state=42)\n",
        "notr_samples     = train_data[train_data[\"label\"] == \"Notr\"].sample(15000, random_state=42)\n",
        "positive_samples = train_data[train_data[\"label\"] == \"Positive\"].sample(15000, random_state=42)\n",
        "\n",
        "# Hepsini birleştir\n",
        "balanced_train = pd.concat([negative_samples, notr_samples, positive_samples]).reset_index(drop=True)\n",
        "\n",
        "# Shuffle / karıştır\n",
        "balanced_train = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Sadece text ve label sütunlarını al\n",
        "balanced_train = balanced_train[['text', 'label']]\n",
        "\n",
        "# Hugging Face Dataset formatına çevir\n",
        "balanced_dataset = Dataset.from_pandas(balanced_train)\n",
        "\n",
        "# Kontrol\n",
        "print(\"Yeni veri seti boyutu:\", len(balanced_dataset))\n",
        "print(\"İlk 10 örnek:\\n\", balanced_dataset[:10])\n"
      ],
      "metadata": {
        "id": "JsvsbWN8hHgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "hmBYX-DehfO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4LZ95Jajhg3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4XuRynHBhmeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FdWpIF6jhqkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Test veri seti (çoklu JSON)\n",
        "# =========================\n",
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Colab'a yüklediğin JSON dosyaları\n",
        "test_json_paths = [\"a1_veri.json\", \"a2_veri.json\", \"b1_veri.json\"]\n",
        "\n",
        "all_test_texts = []\n",
        "all_test_labels = []\n",
        "\n",
        "for path in test_json_paths:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    # JSON içeriğini kontrol et ve text/label ayıkla\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict):\n",
        "            texts = [item.get('text', item.get('sentence', '')) for item in data]\n",
        "            labels = [item.get('label', item.get('sentiment', 0)) for item in data]\n",
        "        else:\n",
        "            texts = data\n",
        "            labels = [0] * len(texts)\n",
        "        all_test_texts.extend(texts)\n",
        "        all_test_labels.extend(labels)\n",
        "\n",
        "# Hugging Face Dataset formatına çevir\n",
        "test_dataset_custom = Dataset.from_dict({\n",
        "    'text': all_test_texts,\n",
        "    'label': all_test_labels\n",
        "})\n",
        "\n",
        "print(f\"Test veri seti başarıyla yüklendi. Toplam örnek sayısı: {len(test_dataset_custom)}\")\n"
      ],
      "metadata": {
        "id": "gPOSQxjshyfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model ve Tokenizer\n",
        "# =========================\n",
        "print(\"\\nModel yükleniyor...\")\n",
        "model_name = \"savasy/bert-base-turkish-sentiment-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parametreleri: {model.num_parameters():,}\")\n"
      ],
      "metadata": {
        "id": "YN5L4IqRh0RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Tokenizasyon\n",
        "# =========================\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizasyon yapılıyor...\")\n",
        "\n",
        "# Balanced eğitim veri setini tokenize et\n",
        "tokenized_train_dataset = balanced_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Test veri setini tokenize et\n",
        "tokenized_test_dataset = test_dataset_custom.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized_train_dataset\n",
        "test_dataset = tokenized_test_dataset\n",
        "\n",
        "print(f\"\\nEğitim veri seti boyutu: {len(train_dataset)}\")\n",
        "print(f\"Test veri seti boyutu: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "Oo5J8jcrh9EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Metrikler\n",
        "# =========================\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1_weighted\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
        "        \"precision_weighted\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
        "        \"recall_weighted\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "55J1AXcciGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(base_path, \"results\"),\n",
        "    eval_strategy=\"steps\",    # Ara değerlendirme adımı\n",
        "    eval_steps=500,               # Her 500 adımda eval yapılacak\n",
        "    save_strategy=\"steps\",        # Checkpoint kaydetme stratejisi\n",
        "    save_steps=500,               # Her 500 adımda model kaydedilecek\n",
        "    learning_rate=3e-5,           # Öğrenme oranı (slightly higher for faster convergence)\n",
        "    per_device_train_batch_size=32,  # Her GPU’da batch boyutu (hız için artırdık)\n",
        "    per_device_eval_batch_size=64,   # Eval batch boyutu (hız için artırdık)\n",
        "    num_train_epochs=3,           # Toplam epoch sayısı\n",
        "    weight_decay=0.01,            # L2 regularizasyonu\n",
        "    warmup_steps=200,             # Linear warmup, 200 adım\n",
        "    load_best_model_at_end=True,  # En iyi modeli otomatik yükle\n",
        "    metric_for_best_model=\"f1_weighted\", # En iyi modelin metriği\n",
        "    greater_is_better=True,       # Metric ne kadar büyükse o kadar iyi\n",
        "    logging_dir=\"./logs\",         # Tensorboard logları\n",
        "    logging_steps=100,            # Her 100 adımda log\n",
        "    save_total_limit=3,           # Sadece son 3 checkpoint saklanır\n",
        "    push_to_hub=False,            # Hugging Face Hub’a gönderme\n",
        "    report_to=None,               # Logging platformu yok\n",
        "    dataloader_num_workers=4,     # DataLoader worker sayısı (CPU hızlandırma)\n",
        "    fp16=True                      # Mixed precision training (GPU hızlandırma)\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,    # 2 evaluation adımı boyunca iyileşme olmazsa dur\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "uEu2YD-qiOr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Fine-tune başlat\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINE-TUNING BAŞLIYOR\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "train_result = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "bWcW-jCeiU4y",
        "outputId": "320c5d63-0563-47bf-fc96-0882d2625bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "FINE-TUNING BAŞLIYOR\n",
            "==================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Training history'yi kaydet ve grafik çiz\n",
        "history = trainer.state.log_history\n",
        "\n",
        "# Training ve validation loss'larını ayır\n",
        "train_logs = [log for log in history if 'loss' in log and 'eval_loss' not in log]\n",
        "eval_logs = [log for log in history if 'eval_loss' in log]\n",
        "\n",
        "# Loss grafiği çiz\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 1. Training ve Validation Loss\n",
        "plt.subplot(2, 2, 1)\n",
        "if train_logs:\n",
        "    train_steps = [log['step'] for log in train_logs]\n",
        "    train_losses = [log['loss'] for log in train_logs]\n",
        "    plt.plot(train_steps, train_losses, label='Training Loss', color='blue')\n",
        "\n",
        "if eval_logs:\n",
        "    eval_steps = [log['step'] for log in eval_logs]\n",
        "    eval_losses = [log['eval_loss'] for log in eval_logs]\n",
        "    plt.plot(eval_steps, eval_losses, label='Validation Loss', color='red')\n",
        "\n",
        "plt.title('Training ve Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 2. F1 Score\n",
        "plt.subplot(2, 2, 2)\n",
        "if eval_logs:\n",
        "    eval_f1 = [log.get('eval_f1_weighted', 0) for log in eval_logs]\n",
        "    plt.plot(eval_steps, eval_f1, label='F1 Score (Weighted)', color='green')\n",
        "\n",
        "plt.title('F1 Score Gelişimi')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 3. Accuracy\n",
        "plt.subplot(2, 2, 3)\n",
        "if eval_logs:\n",
        "    eval_acc = [log.get('eval_accuracy', 0) for log in eval_logs]\n",
        "    plt.plot(eval_steps, eval_acc, label='Accuracy', color='orange')\n",
        "\n",
        "plt.title('Accuracy Gelişimi')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 4. Learning Rate\n",
        "plt.subplot(2, 2, 4)\n",
        "if train_logs:\n",
        "    learning_rates = [log.get('learning_rate', 0) for log in train_logs]\n",
        "    plt.plot(train_steps, learning_rates, label='Learning Rate', color='purple')\n",
        "\n",
        "plt.title('Learning Rate Scheduler')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 12. Final evaluation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL DEĞERLENDIRME\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "final_results = trainer.evaluate()\n",
        "print(\"\\nFinal Validation Metrics:\")\n",
        "for key, value in final_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# 13. Test setinde detaylı prediction\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Sınıf isimleri\n",
        "class_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DETAYLI SINIFLANDIRMA RAPORU\")\n",
        "print(\"=\"*50)\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "# Her sınıf için ayrı metrikler\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "\n",
        "print(\"\\nSınıf Bazında Detaylı Metrikler:\")\n",
        "print(\"-\" * 60)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  Precision: {precision[i]:.4f}\")\n",
        "    print(f\"  Recall: {recall[i]:.4f}\")\n",
        "    print(f\"  F1-Score: {f1[i]:.4f}\")\n",
        "    print(f\"  Support: {support[i]}\")\n",
        "    print()\n",
        "\n",
        "# Genel metrikler\n",
        "print(\"Genel Metrikler:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_true):.4f}\")\n",
        "print(f\"Macro Avg Precision: {np.mean(precision):.4f}\")\n",
        "print(f\"Macro Avg Recall: {np.mean(recall):.4f}\")\n",
        "print(f\"Macro Avg F1-Score: {np.mean(f1):.4f}\")\n",
        "\n",
        "# Weighted averages\n",
        "precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "print(f\"Weighted Avg Precision: {precision_w:.4f}\")\n",
        "print(f\"Weighted Avg Recall: {recall_w:.4f}\")\n",
        "print(f\"Weighted Avg F1-Score: {f1_w:.4f}\")\n",
        "\n",
        "# 14. Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Normalize edilmiş confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 15. Model ve tokenizer kaydet\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL KAYDEDILIYOR\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "save_path = \"./finetuned_turkish_sentiment_3class\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Model config bilgilerini kaydet\n",
        "config_info = {\n",
        "    \"model_name\": model_name,\n",
        "    \"num_labels\": 3,\n",
        "    \"class_names\": class_names,\n",
        "    \"training_samples\": len(train_dataset),\n",
        "    \"test_samples\": len(test_dataset),\n",
        "    \"final_metrics\": final_results,\n",
        "    \"training_args\": {\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "        \"batch_size\": training_args.per_device_train_batch_size,\n",
        "        \"epochs\": training_args.num_train_epochs,\n",
        "        \"weight_decay\": training_args.weight_decay\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{save_path}/training_info.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(config_info, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Model kaydedildi: {save_path}\")\n",
        "print(\"Training bilgileri training_info.json dosyasına kaydedildi\")\n",
        "\n",
        "# 16. Overfitting analizi\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"OVERFITTING ANALİZİ\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if len(eval_logs) > 1:\n",
        "    final_train_loss = train_losses[-1] if train_losses else None\n",
        "    final_eval_loss = eval_losses[-1] if eval_losses else None\n",
        "\n",
        "    if final_train_loss and final_eval_loss:\n",
        "        loss_diff = final_eval_loss - final_train_loss\n",
        "        print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "        print(f\"Final Validation Loss: {final_eval_loss:.4f}\")\n",
        "        print(f\"Loss Farkı: {loss_diff:.4f}\")\n",
        "\n",
        "        if loss_diff > 0.1:\n",
        "            print(\"⚠️  UYARI: Overfitting belirtileri görülüyor!\")\n",
        "        elif loss_diff > 0.05:\n",
        "            print(\"⚠️  DİKKAT: Hafif overfitting olabilir\")\n",
        "        else:\n",
        "            print(\"✅ Overfitting görülmüyor\")\n",
        "\n",
        "    # Validation metriklerinin trendi\n",
        "    if len(eval_f1) > 3:\n",
        "        recent_f1_trend = eval_f1[-3:]\n",
        "        if recent_f1_trend[-1] < recent_f1_trend[0]:\n",
        "            print(\"📉 F1 skoru son dönemde düşüş gösteriyor - Early stopping etkili oldu\")\n",
        "        else:\n",
        "            print(\"📈 F1 skoru istikrarlı artış gösteriyor\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINE-TUNING TAMAMLANDI!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "UFveskal8oZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}