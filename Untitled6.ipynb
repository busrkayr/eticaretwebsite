{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuxeCDjWgr5aT6jeAsTp4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busrkayr/eticaretwebsite/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPNknQBLglFk"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import json\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "NJWie77ug6Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Colab GPU kontrolÃ¼\n",
        "# ======================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"KullanÄ±lan cihaz: {device}\")\n",
        "\n",
        "# Colab ortamÄ±nda, dosya yolunu \"/content\" olarak ayarla\n",
        "base_path = \"/content\""
      ],
      "metadata": {
        "id": "zIQ80PjchEa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Veri setini yÃ¼kle\n",
        "dataset = load_dataset(\"winvoker/turkish-sentiment-analysis-dataset\")\n",
        "train_data = dataset[\"train\"].to_pandas()\n",
        "\n",
        "# Her sÄ±nÄ±ftan 15.000 Ã¶rnek rastgele seÃ§\n",
        "negative_samples = train_data[train_data[\"label\"] == \"Negative\"].sample(15000, random_state=42)\n",
        "notr_samples     = train_data[train_data[\"label\"] == \"Notr\"].sample(15000, random_state=42)\n",
        "positive_samples = train_data[train_data[\"label\"] == \"Positive\"].sample(15000, random_state=42)\n",
        "\n",
        "# Hepsini birleÅŸtir\n",
        "balanced_train = pd.concat([negative_samples, notr_samples, positive_samples]).reset_index(drop=True)\n",
        "\n",
        "# Shuffle / karÄ±ÅŸtÄ±r\n",
        "balanced_train = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Sadece text ve label sÃ¼tunlarÄ±nÄ± al\n",
        "balanced_train = balanced_train[['text', 'label']]\n",
        "\n",
        "# Hugging Face Dataset formatÄ±na Ã§evir\n",
        "balanced_dataset = Dataset.from_pandas(balanced_train)\n",
        "\n",
        "# Kontrol\n",
        "print(\"Yeni veri seti boyutu:\", len(balanced_dataset))\n",
        "print(\"Ä°lk 10 Ã¶rnek:\\n\", balanced_dataset[:10])\n"
      ],
      "metadata": {
        "id": "JsvsbWN8hHgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "hmBYX-DehfO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4LZ95Jajhg3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4XuRynHBhmeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FdWpIF6jhqkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Test veri seti (Ã§oklu JSON)\n",
        "# =========================\n",
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Colab'a yÃ¼klediÄŸin JSON dosyalarÄ±\n",
        "test_json_paths = [\"a1_veri.json\", \"a2_veri.json\", \"b1_veri.json\"]\n",
        "\n",
        "all_test_texts = []\n",
        "all_test_labels = []\n",
        "\n",
        "for path in test_json_paths:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    # JSON iÃ§eriÄŸini kontrol et ve text/label ayÄ±kla\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        if isinstance(data[0], dict):\n",
        "            texts = [item.get('text', item.get('sentence', '')) for item in data]\n",
        "            labels = [item.get('label', item.get('sentiment', 0)) for item in data]\n",
        "        else:\n",
        "            texts = data\n",
        "            labels = [0] * len(texts)\n",
        "        all_test_texts.extend(texts)\n",
        "        all_test_labels.extend(labels)\n",
        "\n",
        "# Hugging Face Dataset formatÄ±na Ã§evir\n",
        "test_dataset_custom = Dataset.from_dict({\n",
        "    'text': all_test_texts,\n",
        "    'label': all_test_labels\n",
        "})\n",
        "\n",
        "print(f\"Test veri seti baÅŸarÄ±yla yÃ¼klendi. Toplam Ã¶rnek sayÄ±sÄ±: {len(test_dataset_custom)}\")\n"
      ],
      "metadata": {
        "id": "gPOSQxjshyfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model ve Tokenizer\n",
        "# =========================\n",
        "print(\"\\nModel yÃ¼kleniyor...\")\n",
        "model_name = \"savasy/bert-base-turkish-sentiment-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parametreleri: {model.num_parameters():,}\")\n"
      ],
      "metadata": {
        "id": "YN5L4IqRh0RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Tokenizasyon\n",
        "# =========================\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizasyon yapÄ±lÄ±yor...\")\n",
        "\n",
        "# Balanced eÄŸitim veri setini tokenize et\n",
        "tokenized_train_dataset = balanced_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Test veri setini tokenize et\n",
        "tokenized_test_dataset = test_dataset_custom.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized_train_dataset\n",
        "test_dataset = tokenized_test_dataset\n",
        "\n",
        "print(f\"\\nEÄŸitim veri seti boyutu: {len(train_dataset)}\")\n",
        "print(f\"Test veri seti boyutu: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "Oo5J8jcrh9EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Metrikler\n",
        "# =========================\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1_weighted\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
        "        \"precision_weighted\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
        "        \"recall_weighted\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "55J1AXcciGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(base_path, \"results\"),\n",
        "    eval_strategy=\"steps\",    # Ara deÄŸerlendirme adÄ±mÄ±\n",
        "    eval_steps=500,               # Her 500 adÄ±mda eval yapÄ±lacak\n",
        "    save_strategy=\"steps\",        # Checkpoint kaydetme stratejisi\n",
        "    save_steps=500,               # Her 500 adÄ±mda model kaydedilecek\n",
        "    learning_rate=3e-5,           # Ã–ÄŸrenme oranÄ± (slightly higher for faster convergence)\n",
        "    per_device_train_batch_size=32,  # Her GPUâ€™da batch boyutu (hÄ±z iÃ§in artÄ±rdÄ±k)\n",
        "    per_device_eval_batch_size=64,   # Eval batch boyutu (hÄ±z iÃ§in artÄ±rdÄ±k)\n",
        "    num_train_epochs=3,           # Toplam epoch sayÄ±sÄ±\n",
        "    weight_decay=0.01,            # L2 regularizasyonu\n",
        "    warmup_steps=200,             # Linear warmup, 200 adÄ±m\n",
        "    load_best_model_at_end=True,  # En iyi modeli otomatik yÃ¼kle\n",
        "    metric_for_best_model=\"f1_weighted\", # En iyi modelin metriÄŸi\n",
        "    greater_is_better=True,       # Metric ne kadar bÃ¼yÃ¼kse o kadar iyi\n",
        "    logging_dir=\"./logs\",         # Tensorboard loglarÄ±\n",
        "    logging_steps=100,            # Her 100 adÄ±mda log\n",
        "    save_total_limit=3,           # Sadece son 3 checkpoint saklanÄ±r\n",
        "    push_to_hub=False,            # Hugging Face Hubâ€™a gÃ¶nderme\n",
        "    report_to=None,               # Logging platformu yok\n",
        "    dataloader_num_workers=4,     # DataLoader worker sayÄ±sÄ± (CPU hÄ±zlandÄ±rma)\n",
        "    fp16=True                      # Mixed precision training (GPU hÄ±zlandÄ±rma)\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,    # 2 evaluation adÄ±mÄ± boyunca iyileÅŸme olmazsa dur\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "uEu2YD-qiOr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Fine-tune baÅŸlat\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINE-TUNING BAÅžLIYOR\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "train_result = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "bWcW-jCeiU4y",
        "outputId": "320c5d63-0563-47bf-fc96-0882d2625bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "FINE-TUNING BAÅžLIYOR\n",
            "==================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Training history'yi kaydet ve grafik Ã§iz\n",
        "history = trainer.state.log_history\n",
        "\n",
        "# Training ve validation loss'larÄ±nÄ± ayÄ±r\n",
        "train_logs = [log for log in history if 'loss' in log and 'eval_loss' not in log]\n",
        "eval_logs = [log for log in history if 'eval_loss' in log]\n",
        "\n",
        "# Loss grafiÄŸi Ã§iz\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 1. Training ve Validation Loss\n",
        "plt.subplot(2, 2, 1)\n",
        "if train_logs:\n",
        "    train_steps = [log['step'] for log in train_logs]\n",
        "    train_losses = [log['loss'] for log in train_logs]\n",
        "    plt.plot(train_steps, train_losses, label='Training Loss', color='blue')\n",
        "\n",
        "if eval_logs:\n",
        "    eval_steps = [log['step'] for log in eval_logs]\n",
        "    eval_losses = [log['eval_loss'] for log in eval_logs]\n",
        "    plt.plot(eval_steps, eval_losses, label='Validation Loss', color='red')\n",
        "\n",
        "plt.title('Training ve Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 2. F1 Score\n",
        "plt.subplot(2, 2, 2)\n",
        "if eval_logs:\n",
        "    eval_f1 = [log.get('eval_f1_weighted', 0) for log in eval_logs]\n",
        "    plt.plot(eval_steps, eval_f1, label='F1 Score (Weighted)', color='green')\n",
        "\n",
        "plt.title('F1 Score GeliÅŸimi')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 3. Accuracy\n",
        "plt.subplot(2, 2, 3)\n",
        "if eval_logs:\n",
        "    eval_acc = [log.get('eval_accuracy', 0) for log in eval_logs]\n",
        "    plt.plot(eval_steps, eval_acc, label='Accuracy', color='orange')\n",
        "\n",
        "plt.title('Accuracy GeliÅŸimi')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 4. Learning Rate\n",
        "plt.subplot(2, 2, 4)\n",
        "if train_logs:\n",
        "    learning_rates = [log.get('learning_rate', 0) for log in train_logs]\n",
        "    plt.plot(train_steps, learning_rates, label='Learning Rate', color='purple')\n",
        "\n",
        "plt.title('Learning Rate Scheduler')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 12. Final evaluation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL DEÄžERLENDIRME\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "final_results = trainer.evaluate()\n",
        "print(\"\\nFinal Validation Metrics:\")\n",
        "for key, value in final_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# 13. Test setinde detaylÄ± prediction\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# SÄ±nÄ±f isimleri\n",
        "class_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DETAYLI SINIFLANDIRMA RAPORU\")\n",
        "print(\"=\"*50)\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "# Her sÄ±nÄ±f iÃ§in ayrÄ± metrikler\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "\n",
        "print(\"\\nSÄ±nÄ±f BazÄ±nda DetaylÄ± Metrikler:\")\n",
        "print(\"-\" * 60)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  Precision: {precision[i]:.4f}\")\n",
        "    print(f\"  Recall: {recall[i]:.4f}\")\n",
        "    print(f\"  F1-Score: {f1[i]:.4f}\")\n",
        "    print(f\"  Support: {support[i]}\")\n",
        "    print()\n",
        "\n",
        "# Genel metrikler\n",
        "print(\"Genel Metrikler:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_true):.4f}\")\n",
        "print(f\"Macro Avg Precision: {np.mean(precision):.4f}\")\n",
        "print(f\"Macro Avg Recall: {np.mean(recall):.4f}\")\n",
        "print(f\"Macro Avg F1-Score: {np.mean(f1):.4f}\")\n",
        "\n",
        "# Weighted averages\n",
        "precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "print(f\"Weighted Avg Precision: {precision_w:.4f}\")\n",
        "print(f\"Weighted Avg Recall: {recall_w:.4f}\")\n",
        "print(f\"Weighted Avg F1-Score: {f1_w:.4f}\")\n",
        "\n",
        "# 14. Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Normalize edilmiÅŸ confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 15. Model ve tokenizer kaydet\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL KAYDEDILIYOR\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "save_path = \"./finetuned_turkish_sentiment_3class\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Model config bilgilerini kaydet\n",
        "config_info = {\n",
        "    \"model_name\": model_name,\n",
        "    \"num_labels\": 3,\n",
        "    \"class_names\": class_names,\n",
        "    \"training_samples\": len(train_dataset),\n",
        "    \"test_samples\": len(test_dataset),\n",
        "    \"final_metrics\": final_results,\n",
        "    \"training_args\": {\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "        \"batch_size\": training_args.per_device_train_batch_size,\n",
        "        \"epochs\": training_args.num_train_epochs,\n",
        "        \"weight_decay\": training_args.weight_decay\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{save_path}/training_info.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(config_info, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Model kaydedildi: {save_path}\")\n",
        "print(\"Training bilgileri training_info.json dosyasÄ±na kaydedildi\")\n",
        "\n",
        "# 16. Overfitting analizi\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"OVERFITTING ANALÄ°ZÄ°\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if len(eval_logs) > 1:\n",
        "    final_train_loss = train_losses[-1] if train_losses else None\n",
        "    final_eval_loss = eval_losses[-1] if eval_losses else None\n",
        "\n",
        "    if final_train_loss and final_eval_loss:\n",
        "        loss_diff = final_eval_loss - final_train_loss\n",
        "        print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "        print(f\"Final Validation Loss: {final_eval_loss:.4f}\")\n",
        "        print(f\"Loss FarkÄ±: {loss_diff:.4f}\")\n",
        "\n",
        "        if loss_diff > 0.1:\n",
        "            print(\"âš ï¸  UYARI: Overfitting belirtileri gÃ¶rÃ¼lÃ¼yor!\")\n",
        "        elif loss_diff > 0.05:\n",
        "            print(\"âš ï¸  DÄ°KKAT: Hafif overfitting olabilir\")\n",
        "        else:\n",
        "            print(\"âœ… Overfitting gÃ¶rÃ¼lmÃ¼yor\")\n",
        "\n",
        "    # Validation metriklerinin trendi\n",
        "    if len(eval_f1) > 3:\n",
        "        recent_f1_trend = eval_f1[-3:]\n",
        "        if recent_f1_trend[-1] < recent_f1_trend[0]:\n",
        "            print(\"ðŸ“‰ F1 skoru son dÃ¶nemde dÃ¼ÅŸÃ¼ÅŸ gÃ¶steriyor - Early stopping etkili oldu\")\n",
        "        else:\n",
        "            print(\"ðŸ“ˆ F1 skoru istikrarlÄ± artÄ±ÅŸ gÃ¶steriyor\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINE-TUNING TAMAMLANDI!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "UFveskal8oZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}